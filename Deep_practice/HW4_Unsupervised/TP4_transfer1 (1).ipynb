{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"TP4_transfer.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"881e5d299e1d49b2aaff6bb9ff77e458":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f4147069a7944085bf7b7237e3810b92","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c7d554cfafb14b8e82902bbca7517e7c","IPY_MODEL_98005e30daa74ff98df6586c239f1c87"]}},"f4147069a7944085bf7b7237e3810b92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c7d554cfafb14b8e82902bbca7517e7c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1658c7f13d8d488baf2a054e96a7791f","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":46827520,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":46827520,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_de8c034784284981952e1478b4c57c60"}},"98005e30daa74ff98df6586c239f1c87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_785fb8010148467b8ad4b6dcd27c8d7a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 44.7M/44.7M [04:42&lt;00:00, 166kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9979472e01f141faa3b3c7471ab9129a"}},"1658c7f13d8d488baf2a054e96a7791f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"de8c034784284981952e1478b4c57c60":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"785fb8010148467b8ad4b6dcd27c8d7a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9979472e01f141faa3b3c7471ab9129a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ai62aof6QFwI"},"source":["# Small data and deep learning\n","This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."]},{"cell_type":"markdown","metadata":{"id":"_s2y846fQFwS"},"source":["# Introduction\n","Assume we are in a context where few \"gold\" labeled data are available for training, say $\\mathcal{X}_{\\text{train}}\\triangleq\\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$, where $N_{\\text{train}}$ is small. A large test set $\\mathcal{X}_{\\text{test}}$ is available. A large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n","\n","For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question:\n","\n","| Model | Number of  epochs  | Train accuracy | Test accuracy |\n","|------|------|------|------|\n","|   XXX  | XXX | XXX | XXX |\n","\n","If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset)\n","\n","In your final report, please keep the logs of each training procedure you used. We will only run this jupyter if we have some doubts on your implementation. \n","\n","__The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.__\n","\n","\n","You can use https://colab.research.google.com/ to run your experiments."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aI9V1Mj0QFwT","executionInfo":{"status":"ok","timestamp":1613669309008,"user_tz":-60,"elapsed":1215,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"4b720700-e9f3-4b7a-95d1-e9a4ef2a2131"},"source":["import os\n","\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import datasets, transforms\n","from tqdm import tqdm\n","\n","DATA_FOLDER = \".\"\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available else \"cpu\")\n","device"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"LHa-1uV0QFwV"},"source":["## Training set creation\n","__Question 1 (2 points):__ Propose a dataloader or modify the file located at https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py in order to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xliuPXcAQFwW","executionInfo":{"status":"ok","timestamp":1613669312131,"user_tz":-60,"elapsed":1521,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"22459d63-5f3c-45e2-c697-9f7de998f928"},"source":["class SubDataset(torch.utils.data.Dataset):\n","    def __init__(self, dataset, indices):\n","        self.dataset = dataset\n","        self.indices = list(indices)\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        return self.dataset[self.indices[idx]]\n","\n","    def __repr__(self):\n","        upper_repr = repr(self.dataset)\n","        upper_repr = \"\\n    \".join(upper_repr.split(\"\\n\"))\n","        return f\"SubDataset of length {len(self)} from\\n    {upper_repr}\"\n","\n","\n","test = True\n","if test:\n","    X_train = datasets.CIFAR10(root=DATA_FOLDER, train=True, download=True)\n","\n","    X = SubDataset(X_train, range(100, len(X_train)))\n","    X_train = SubDataset(X_train, range(100))\n","    print(X_train, X, sep=\"\\n\")\n","    del X_train, X"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","SubDataset of length 100 from\n","    Dataset CIFAR10\n","        Number of datapoints: 50000\n","        Root location: .\n","        Split: Train\n","SubDataset of length 49900 from\n","    Dataset CIFAR10\n","        Number of datapoints: 50000\n","        Root location: .\n","        Split: Train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"M0fNs_d-QFwY"},"source":["from functools import reduce\n","\n","def f(d, elt):\n","    c = d.get(elt, 0)\n","    d[elt] = c + 1\n","    return d\n","\n","d = reduce(f, map(lambda x: x[1], list(X_train)), {})\n","sorted(d.items())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExTHn5znQFwY"},"source":["This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project. The remaining samples correspond to $\\mathcal{X}$. The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."]},{"cell_type":"markdown","metadata":{"id":"EDM7TXM5QFwZ"},"source":["## Testing procedure\n","__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."]},{"cell_type":"markdown","metadata":{"id":"ot-XZpFnQFwZ"},"source":["It's hard to stop the learning based on a validation set."]},{"cell_type":"markdown","metadata":{"id":"kyDvubjDQFwa"},"source":["# Raw approach: the baseline"]},{"cell_type":"markdown","metadata":{"id":"EUROc22AQFwa"},"source":["In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n","\n","The key ingredients for training a CNN are the batch size, as well as the learning rate schedule, i.e. how to decrease the learning rate as a function of the number of epochs. A possible schedule is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the laerning rate. A potential batch size could be 10, yet this can be cross-validated.\n","\n","You can get some baselines accuracies in this paper: http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf. Obviously, it is a different context for those researchers who had access to GPUs."]},{"cell_type":"code","metadata":{"id":"MiKz5PXvQFwc"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P9luaDA7QFwc"},"source":["## ResNet architectures"]},{"cell_type":"markdown","metadata":{"id":"zORRgsjXQFwc"},"source":["__Question 3 (4 points):__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n","\n","*Hint:* You can re-use the following code: https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."]},{"cell_type":"code","metadata":{"id":"oiYcAs9xQFwc","executionInfo":{"status":"ok","timestamp":1613669326746,"user_tz":-60,"elapsed":8803,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["## DATA ##\n","\n","cifar_train = datasets.CIFAR10(root=DATA_FOLDER, train=True, transform=transforms.ToTensor())\n","\n","mean = torch.zeros(3)\n","std = torch.zeros(3)\n","for img, _ in cifar_train:\n","    mean += img.mean(dim=[1, 2])\n","    std += img.std(dim=[1, 2])\n","mean /= len(cifar_train)\n","std /= len(cifar_train)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32EVIj5wQx2Q","executionInfo":{"status":"ok","timestamp":1613669328203,"user_tz":-60,"elapsed":7907,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"8b306927-96c9-4b10-937d-a5df34801ba8"},"source":["# No data augmentation\n","\n","train_transform = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","test_transform = torchvision.transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","X_train = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=True, transform=train_transform, download=True)\n","X_train = SubDataset(X_train, range(100))\n","X_test = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=False, transform=test_transform, download=True)\n","\n","train_loader = torch.utils.data.DataLoader(X_train, 10, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(X_test, 256, shuffle=False)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-oo4PIvSKx3Y","executionInfo":{"status":"ok","timestamp":1613669343875,"user_tz":-60,"elapsed":516,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["# Loss / accuracy\n","criterion = nn.CrossEntropyLoss()\n","\n","def accuracy(prediction, target):\n","    prediction = prediction.argmax(dim=1)\n","    return (prediction == target).sum() / len(target)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YR0PahARQFwc","executionInfo":{"status":"ok","timestamp":1613669358461,"user_tz":-60,"elapsed":12037,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"7f57055d-2c9d-4751-a073-3df91a3f3ec7"},"source":["## MODEL ##\n","# Code from pytorch-cifar github (https://github.com/kuangliu/pytorch-cifar)\n","\n","class BasicBlock(nn.Module):\n","    expansion = 1\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(BasicBlock, self).__init__()\n","        self.conv1 = nn.Conv2d(\n","            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.bn2(self.conv2(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class Bottleneck(nn.Module):\n","    expansion = 4\n","\n","    def __init__(self, in_planes, planes, stride=1):\n","        super(Bottleneck, self).__init__()\n","        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(planes)\n","        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n","                               stride=stride, padding=1, bias=False)\n","        self.bn2 = nn.BatchNorm2d(planes)\n","        self.conv3 = nn.Conv2d(planes, self.expansion *\n","                               planes, kernel_size=1, bias=False)\n","        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n","\n","        self.shortcut = nn.Sequential()\n","        if stride != 1 or in_planes != self.expansion*planes:\n","            self.shortcut = nn.Sequential(\n","                nn.Conv2d(in_planes, self.expansion*planes,\n","                          kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(self.expansion*planes)\n","            )\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = F.relu(self.bn2(self.conv2(out)))\n","        out = self.bn3(self.conv3(out))\n","        out += self.shortcut(x)\n","        out = F.relu(out)\n","        return out\n","\n","\n","class ResNet(nn.Module):\n","    def __init__(self, block, num_blocks, num_classes=10):\n","        super(ResNet, self).__init__()\n","        self.in_planes = 64\n","\n","        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n","                               stride=1, padding=1, bias=False)\n","        self.bn1 = nn.BatchNorm2d(64)\n","        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n","        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n","        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n","        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n","        self.linear = nn.Linear(512*block.expansion, num_classes)\n","\n","    def _make_layer(self, block, planes, num_blocks, stride):\n","        strides = [stride] + [1]*(num_blocks-1)\n","        layers = []\n","        for stride in strides:\n","            layers.append(block(self.in_planes, planes, stride))\n","            self.in_planes = planes * block.expansion\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        out = F.relu(self.bn1(self.conv1(x)))\n","        out = self.layer1(out)\n","        out = self.layer2(out)\n","        out = self.layer3(out)\n","        out = self.layer4(out)\n","        out = F.avg_pool2d(out, 4)\n","        out = out.view(out.size(0), -1)\n","        out = self.linear(out)\n","        return out\n","\n","\n","def ResNet18():\n","    return ResNet(BasicBlock, [2, 2, 2, 2])\n","\n","model = ResNet18()\n","model.to(device)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"lNSkaHN-QFwd","executionInfo":{"status":"ok","timestamp":1613669358960,"user_tz":-60,"elapsed":490,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"SkT52OB4QFwe","executionInfo":{"status":"ok","timestamp":1613669358964,"user_tz":-60,"elapsed":490,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["def train(model, dataloader, optimizer, criterion, device):\n","    total_loss = 0\n","    N = 0\n","    for i, (data, target) in enumerate(dataloader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        prediction = model(data)\n","        loss = criterion(prediction, target)\n","        \n","        batch_size = data.shape[0]\n","        N += batch_size\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss * batch_size\n","    return total_loss / N\n","\n","def validate(model, dataloader, criterion, device):\n","    with torch.no_grad():\n","        total_loss = 0\n","        N = 0\n","        for i, (data, target) in enumerate(dataloader):\n","            data = data.to(device)\n","            target = target.to(device)\n","            \n","            batch_size = data.shape[0]\n","            N += batch_size\n","\n","            prediction = model(data)\n","            total_loss += criterion(prediction, target) * batch_size\n","    return total_loss / N"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M4MQPn7lQFwe","executionInfo":{"status":"ok","timestamp":1613669365753,"user_tz":-60,"elapsed":4797,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"56438ddf-4d52-45cc-9d99-c25ce1df2a6f"},"source":["for i in range(10): # up to 23% (Even with more epochs)\n","    print(i, end=\"...\", flush=True)\n","    loss = train(model, train_loader, optimizer, criterion, device).item()\n","    print(f\"\\r{i}: {loss:.4f}\")\n","\n","validate(model, train_loader, accuracy, device).item(), validate(model, test_loader, accuracy, device).item()"],"execution_count":9,"outputs":[{"output_type":"stream","text":["0: 2.8355\n","1: 2.4048\n","2: 1.9479\n","3: 1.6552\n","4: 1.4052\n","5: 1.2381\n","6: 1.0629\n","7: 0.9017\n","8: 1.2125\n","9: 0.9551\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(0.7099999785423279, 0.22809998691082)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"U_6Gp7RrQFwf"},"source":["# Transfer learning"]},{"cell_type":"markdown","metadata":{"id":"d9NcX7dXQFwf"},"source":["We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."]},{"cell_type":"markdown","metadata":{"id":"i76SI4rlQFwf"},"source":["## ImageNet features"]},{"cell_type":"markdown","metadata":{"id":"fiaN-SlhQFwf"},"source":["Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on: https://pytorch.org/docs/stable/torchvision/models.html.\n","\n","__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-a1GoCmbdARK","executionInfo":{"status":"ok","timestamp":1613669391327,"user_tz":-60,"elapsed":3194,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"b423b1a6-d43c-4d56-ee0a-cc042e0d378f"},"source":["transform = torchvision.transforms.Compose([\n","    transforms.Resize(224),  # Important to resize in order to be able to use pretrained convolutions\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","X_train = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=True, transform=transform, download=True)\n","X_train = SubDataset(X_train, range(100))\n","X_test = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=False, transform=transform, download=True)\n","train_loader = torch.utils.data.DataLoader(X_train, 10, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(X_test, 256, shuffle=False)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["881e5d299e1d49b2aaff6bb9ff77e458","f4147069a7944085bf7b7237e3810b92","c7d554cfafb14b8e82902bbca7517e7c","98005e30daa74ff98df6586c239f1c87","1658c7f13d8d488baf2a054e96a7791f","de8c034784284981952e1478b4c57c60","785fb8010148467b8ad4b6dcd27c8d7a","9979472e01f141faa3b3c7471ab9129a"]},"id":"PZxv3wigQFwf","executionInfo":{"status":"ok","timestamp":1613669394809,"user_tz":-60,"elapsed":987,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"5756d831-d9e9-41a2-ba04-087a35710b5b"},"source":["model = torchvision.models.resnet18(pretrained=True)\n","model.fc = nn.Linear(512, 10)\n","model.to(device)"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"881e5d299e1d49b2aaff6bb9ff77e458","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=46827520.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ApqRQtoFQFwg","executionInfo":{"status":"ok","timestamp":1613669435063,"user_tz":-60,"elapsed":25864,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"0e346127-3da9-4ee6-f5bc-faeb4b2618e4"},"source":["# Train only last fc layer\n","\n","optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001)\n","# optimizer = torch.optim.Adam(\n","#     [{\"params\": model.layer4.parameters(), \"lr\": 0.0001}, {\"params\": model.fc.parameters(), \"lr\": 0.01}],\n","#     )\n","\n","\n","for i in range(20): # Up to 55 %\n","    print(i, end=\"...\", flush=True)\n","    loss = train(model, train_loader, optimizer, criterion, device).item()\n","    print(f\"\\r{i}: {loss:.4f}\")\n","\n","validate(model, train_loader, accuracy, device).item(), validate(model, test_loader, accuracy, device).item()"],"execution_count":14,"outputs":[{"output_type":"stream","text":["0: 2.3207\n","1: 1.9636\n","2: 1.7445\n","3: 1.5425\n","4: 1.4172\n","5: 1.2150\n","6: 1.1329\n","7: 1.0485\n","8: 1.0012\n","9: 0.8833\n","10: 0.8115\n","11: 0.7767\n","12: 0.6308\n","13: 0.6391\n","14: 0.5653\n","15: 0.5215\n","16: 0.5308\n","17: 0.5095\n","18: 0.5269\n","19: 0.4266\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(0.949999988079071, 0.5120999813079834)"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"zclO1Q04QFwg","executionInfo":{"status":"ok","timestamp":1613657438457,"user_tz":-60,"elapsed":522,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":[],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p122TSLNQFwg","executionInfo":{"status":"ok","timestamp":1613669504273,"user_tz":-60,"elapsed":22883,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"1cf61615-4dcb-40cd-cc1c-9d3d9334031b"},"source":["# Retrain a bit all the network\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n","\n","for i in range(10): # Up to 60 %\n","    print(i, end=\"...\", flush=True)\n","    loss = train(model, train_loader, optimizer, criterion, device).item()\n","    print(f\"\\r{i}: {loss:.4f}\")\n","\n","validate(model, train_loader, accuracy, device).item(), validate(model, test_loader, accuracy, device).item()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0: 0.4735\n","1: 0.0757\n","2: 0.0426\n","3: 0.0246\n","4: 0.0110\n","5: 0.0081\n","6: 0.0083\n","7: 0.0059\n","8: 0.0194\n","9: 0.0044\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(1.0, 0.589199960231781)"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"9IbtFp7-QFwh"},"source":["# Incorporating *a priori*\n","Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that:\n","\n","$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n","\n","For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to:\n","\n","$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n","\n","Otherwise, one has to handle several boundary effects.\n","\n","__Question 5 (1.5 points):__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."]},{"cell_type":"markdown","metadata":{"id":"t-ssWSM1QFwh"},"source":[]},{"cell_type":"markdown","metadata":{"id":"9MyEvt98QFwh"},"source":["## Data augmentations"]},{"cell_type":"markdown","metadata":{"id":"vYnx1hifQFwh"},"source":["__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."]},{"cell_type":"code","metadata":{"id":"PBk5Lx7NQFwi","executionInfo":{"status":"ok","timestamp":1613669912151,"user_tz":-60,"elapsed":1396,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["fill_color = tuple(map(lambda x: int(255*x), mean))\n","\n","augmented_transform = transforms.Compose([\n","    transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomApply([transforms.RandomRotation(20, fill=fill_color)], 0.3),\n","    # transforms.RandomApply([transforms.GaussianBlur(35, 0.5)], 0.5),\n","#     transforms.RandomCrop(32, 4, padding_mode=\"reflect\"),\n","#     transforms.RandomPerspective,\n","    transforms.RandomApply([transforms.RandomResizedCrop(32, scale=(0.6, 1.0))], 0.7),\n","    # transforms.GaussianBlur(7, 0.5),\n","    # transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","    # transforms.RandomApply([transforms.RandomErasing(1, scale=(0.01, 0.1))], 0.5),\n","])\n","\n","base_transform = torchvision.transforms.Compose([\n","    # transforms.Resize(224),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean, std),\n","])\n","\n","X_train = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=True, transform=augmented_transform)\n","X_train = SubDataset(X_train, range(100))\n","X_test = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=False, transform=base_transform)\n","train_loader = torch.utils.data.DataLoader(X_train, 10, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(X_test, 256, shuffle=False)"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"zhd6AFEzn1Mw","executionInfo":{"status":"ok","timestamp":1613669671450,"user_tz":-60,"elapsed":1353,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}}},"source":["X = torchvision.datasets.CIFAR10(root=DATA_FOLDER, train=True, transform=base_transform)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"IPkT01ZRn97Z","executionInfo":{"status":"ok","timestamp":1613669854893,"user_tz":-60,"elapsed":977,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"9615960b-4d85-4ca8-9c8b-9b5596eeba02"},"source":["# Can be used to visualized transforms. Better without the normalize transformation\n","\n","i = 12\n","img_base = X[i][0]\n","img_aug = X_train[i][0]\n","\n","plt.imshow(torch.cat((img_base, img_aug), dim=2).permute(1, 2, 0))\n","plt.show()"],"execution_count":48,"outputs":[{"output_type":"stream","text":["Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"],"name":"stderr"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ard3YrwvQFwi","executionInfo":{"status":"ok","timestamp":1613670011956,"user_tz":-60,"elapsed":482,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"203ec004-be90-4a74-e025-f29d0fd14d50"},"source":["model = ResNet18()\n","model.to(device)"],"execution_count":63,"outputs":[{"output_type":"execute_result","data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (shortcut): Sequential()\n","    )\n","  )\n","  (linear): Linear(in_features=512, out_features=10, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":63}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WWCaTvSXQFwj","executionInfo":{"status":"ok","timestamp":1613670032037,"user_tz":-60,"elapsed":15269,"user":{"displayName":"Raphael Reme","photoUrl":"","userId":"10536219480337117409"}},"outputId":"aaba4455-5d33-42d6-ccd1-094d7bd24ce3"},"source":["optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n","\n","for i in range(50): # Up to 28 %\n","    print(i, end=\"...\", flush=True)\n","    loss = train(model, train_loader, optimizer, criterion, device).item()\n","    print(f\"\\r{i}: {loss:.4f}\")\n","\n","validate(model, train_loader, accuracy, device).item(), validate(model, test_loader, accuracy, device).item()"],"execution_count":64,"outputs":[{"output_type":"stream","text":["0: 2.7496\n","1: 2.3637\n","2: 2.2251\n","3: 2.0085\n","4: 1.9373\n","5: 1.7671\n","6: 1.7133\n","7: 1.7461\n","8: 1.6653\n","9: 1.6426\n","10: 1.5069\n","11: 1.5528\n","12: 1.4958\n","13: 1.3727\n","14: 1.2754\n","15: 1.2313\n","16: 1.4386\n","17: 1.2862\n","18: 1.3034\n","19: 1.1444\n","20: 1.0872\n","21: 1.1194\n","22: 1.1152\n","23: 1.0849\n","24: 0.9297\n","25: 0.8954\n","26: 0.7377\n","27: 0.9043\n","28: 0.7181\n","29: 0.7432\n","30: 0.9049\n","31: 0.8038\n","32: 0.8246\n","33: 0.8738\n","34: 0.8104\n","35: 0.6183\n","36: 0.6318\n","37: 0.5546\n","38: 0.7901\n","39: 0.6723\n","40: 0.7467\n","41: 0.6669\n","42: 0.5169\n","43: 0.6335\n","44: 0.6487\n","45: 0.5401\n","46: 0.5990\n","47: 0.5506\n","48: 0.5558\n","49: 0.4641\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(0.8299999833106995, 0.2775999903678894)"]},"metadata":{"tags":[]},"execution_count":64}]},{"cell_type":"markdown","metadata":{"id":"fyX8iEtbQFwj"},"source":["# Conclusions"]},{"cell_type":"markdown","metadata":{"id":"41szX6umQFwj"},"source":["__Question 7 (5 points):__ Write a short report explaining the pros and the cons of each methods that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."]},{"cell_type":"markdown","metadata":{"id":"32cUqwA4QFwj"},"source":[]},{"cell_type":"markdown","metadata":{"id":"xNELzfFpQFwk"},"source":["# Weak supervision"]},{"cell_type":"markdown","metadata":{"id":"ary5SCXZQFwk"},"source":["__Bonus \\[open\\] question (up to 3 points):__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class cat_dataloader():\n","    \"\"\"Class to concatenate multiple dataloaders\"\"\"\n","\n","    def __init__(self, dataloaders):\n","        self.dataloaders = dataloaders\n","        len(self.dataloaders)\n","\n","    def __iter__(self):\n","        self.loader_iter = []\n","        for data_loader in self.dataloaders:\n","            self.loader_iter.append(iter(data_loader))\n","        return self\n","\n","    def __next__(self):\n","        out = []\n","        for data_iter in self.loader_iter:\n","            out.append(next(data_iter)) # may raise StopIteration\n","        return tuple(out)\n","\n","\n","ssl_loader= cat_dataloader([train_loader,test_loader])\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion, device):\n","    total_loss = 0\n","    N = 0\n","    for i, (data, target) in enumerate(dataloader):\n","        data = data.to(device)\n","        target = target.to(device)\n","\n","        prediction = model(data)\n","        loss = criterion(prediction, target)\n","        \n","        batch_size = data.shape[0]\n","        N += batch_size\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        total_loss += loss * batch_size\n","    return total_loss / N"]},{"cell_type":"markdown","metadata":{"id":"RpNyFdJ5QFwk"},"source":[]}]}